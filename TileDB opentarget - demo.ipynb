{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import tiledb\n",
    "tiledb.libtiledb.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# import SparkContext and SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-8V7ENTJ:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x145af6a8400>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"spark test\")\n",
    "sc = SparkContext(conf = conf)\n",
    "ss = SparkSession.builder.master('local').appName('spark test')\\\n",
    "    .getOrCreate()\n",
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TileDB from pySpark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read json data into pySpark dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After I downgrade spark 3.1.1 to 2.4.4 and python 3.7.9 to 3.6. There are no confilict for pyspark api, `tiledb-spark-0.5.3.jar`, with native pySpark json/csv reader and writer. I can sucessfully read the json file and write csv. So my previous problem seems to be version related, or how the spark is discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "study_index = ss.read.json('data/study_index_sample.json')\n",
    "# study_index = ss.read.format(\"org.apache.spark.sql.execution.datasources.json.JsonFileFormat\")\\\n",
    "#                 .load('data/study_index_sample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_index.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+-------+---------+-------------+--------------+-------------+-------------+----------+--------------+--------------------+----------+--------------------+-------------+--------------------+\n",
      "|    ancestry_initial|ancestry_replication|has_sumstats|n_cases|n_initial|n_replication|num_assoc_loci|         pmid|   pub_author|  pub_date|   pub_journal|           pub_title|  study_id|      trait_category|   trait_efos|      trait_reported|\n",
      "+--------------------+--------------------+------------+-------+---------+-------------+--------------+-------------+-------------+----------+--------------+--------------------+----------+--------------------+-------------+--------------------+\n",
      "|      [European=146]|                null|       false|     96|      146|         null|             1|PMID:15761122|     Klein RJ|2005-03-10|       Science|Complement factor...|GCST000001|      Nervous system|[EFO_0001365]|Age-related macul...|\n",
      "|[Asian unspecifie...|[Other=122, Asian...|       false|   null|      886|          664|             0|PMID:16252231|Maraganore DM|2005-09-09|Am J Hum Genet|High-resolution w...|GCST000002|      Nervous system|[EFO_0002508]| Parkinson's disease|\n",
      "|            [NR=694]|[European=10527, ...|       false|   null|      694|        11622|             0|PMID:16614226|    Herbert A|2006-04-14|       Science|A common genetic ...|GCST000003|             Disease|[EFO_0001073]|             Obesity|\n",
      "|      [European=200]|     [European=4851]|       false|   null|      200|         4851|             1|PMID:16648850|    Arking DE|2006-04-30|     Nat Genet|A common genetic ...|GCST000004|Cardiovascular me...|[EFO_0004682]|         QT interval|\n",
      "|      [European=537]|                null|       false|    267|      537|         null|             0|PMID:17052657|      Fung HC|2006-09-28| Lancet Neurol|Genome-wide genot...|GCST000005|      Nervous system|[EFO_0002508]| Parkinson's disease|\n",
      "+--------------------+--------------------+------------+-------+---------+-------------+--------------+-------------+-------------+----------+--------------+--------------------+----------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "study_index.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOOK at the SCHEMA. These are:\n",
    "- `String Arrays` for attributes: ancestry_initial, ancestry_replication, trait_efos \n",
    "- `Boolean` for attribute: has_sumstats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ancestry_initial: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- ancestry_replication: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- has_sumstats: boolean (nullable = true)\n",
      " |-- n_cases: long (nullable = true)\n",
      " |-- n_initial: long (nullable = true)\n",
      " |-- n_replication: long (nullable = true)\n",
      " |-- num_assoc_loci: long (nullable = true)\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- pub_author: string (nullable = true)\n",
      " |-- pub_date: string (nullable = true)\n",
      " |-- pub_journal: string (nullable = true)\n",
      " |-- pub_title: string (nullable = true)\n",
      " |-- study_id: string (nullable = true)\n",
      " |-- trait_category: string (nullable = true)\n",
      " |-- trait_efos: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- trait_reported: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "study_index.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TileDB Array from pySpark Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the documentation, TileDB can be create directly from Spark Dataframe directly. But there seems to be some version compatability issues. And also there are much less options and flexibilities, such as the empty values. Therefore, here we write Spark dataframe to csv and then create TileDB from CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NullPointer exception while writing your data. It seems that there is a bug in our implementation. As a temporary solution and to help you get unstuck, try converting all types to String. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import concat, concat_ws, lit, col, trim\n",
    "# from pyspark.sql.types import StringType\n",
    "# for each_col in study_index.columns:\n",
    "#     study_index = study_index.withColumn(each_col,col(each_col).cast(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, concat_ws, lit, col, trim\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "study_index= study_index.withColumn(\"trait_efos\",col(\"trait_efos\").cast(StringType()))\n",
    "study_index= study_index.withColumn(\"ancestry_replication\",col(\"ancestry_replication\").cast(StringType()))\n",
    "study_index= study_index.withColumn(\"ancestry_initial\",col(\"ancestry_initial\").cast(StringType()))\n",
    "study_index = study_index.withColumn(\"has_sumstats\",col(\"has_sumstats\").cast(StringType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ancestry_initial: string (nullable = true)\n",
      " |-- ancestry_replication: string (nullable = true)\n",
      " |-- has_sumstats: string (nullable = true)\n",
      " |-- n_cases: long (nullable = true)\n",
      " |-- n_initial: long (nullable = true)\n",
      " |-- n_replication: long (nullable = true)\n",
      " |-- num_assoc_loci: long (nullable = true)\n",
      " |-- pmid: string (nullable = true)\n",
      " |-- pub_author: string (nullable = true)\n",
      " |-- pub_date: string (nullable = true)\n",
      " |-- pub_journal: string (nullable = true)\n",
      " |-- pub_title: string (nullable = true)\n",
      " |-- study_id: string (nullable = true)\n",
      " |-- trait_category: string (nullable = true)\n",
      " |-- trait_efos: string (nullable = true)\n",
      " |-- trait_reported: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "study_index.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o207.save.\n: org.apache.spark.SparkException: Writing job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:92)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.lang.NullPointerException\r\n\tat io.tiledb.spark.TileDBDataWriter.writeRecordToBuffer(TileDBDataWriter.java:242)\r\n\tat io.tiledb.spark.TileDBDataWriter.bufferAttributeValue(TileDBDataWriter.java:220)\r\n\tat io.tiledb.spark.TileDBDataWriter.write(TileDBDataWriter.java:718)\r\n\tat io.tiledb.spark.TileDBDataWriter.write(TileDBDataWriter.java:32)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:64)\r\n\t... 26 more\r\nCaused by: java.lang.NullPointerException\r\n\tat io.tiledb.spark.TileDBDataWriter.writeRecordToBuffer(TileDBDataWriter.java:242)\r\n\tat io.tiledb.spark.TileDBDataWriter.bufferAttributeValue(TileDBDataWriter.java:220)\r\n\tat io.tiledb.spark.TileDBDataWriter.write(TileDBDataWriter.java:718)\r\n\tat io.tiledb.spark.TileDBDataWriter.write(TileDBDataWriter.java:32)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-356519ead94e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"io.tiledb.spark\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"uri\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"study_index_tldb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m   \u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'schema.dim.0.name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'study_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m   \u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\spark\\spark-3.1.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o207.save.\n: org.apache.spark.SparkException: Writing job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:92)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:260)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4, localhost, executor driver): java.lang.NullPointerException\r\n\tat io.tiledb.spark.TileDBDataWriter.writeRecordToBuffer(TileDBDataWriter.java:242)\r\n\tat io.tiledb.spark.TileDBDataWriter.bufferAttributeValue(TileDBDataWriter.java:220)\r\n\tat io.tiledb.spark.TileDBDataWriter.write(TileDBDataWriter.java:718)\r\n\tat io.tiledb.spark.TileDBDataWriter.write(TileDBDataWriter.java:32)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.doExecute(WriteToDataSourceV2Exec.scala:64)\r\n\t... 26 more\r\nCaused by: java.lang.NullPointerException\r\n\tat io.tiledb.spark.TileDBDataWriter.writeRecordToBuffer(TileDBDataWriter.java:242)\r\n\tat io.tiledb.spark.TileDBDataWriter.bufferAttributeValue(TileDBDataWriter.java:220)\r\n\tat io.tiledb.spark.TileDBDataWriter.write(TileDBDataWriter.java:718)\r\n\tat io.tiledb.spark.TileDBDataWriter.write(TileDBDataWriter.java:32)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:118)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$$anonfun$run$3.apply(WriteToDataSourceV2Exec.scala:116)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\r\n\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:67)\r\n\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$$anonfun$doExecute$2.apply(WriteToDataSourceV2Exec.scala:66)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "study_index.write\\\n",
    "  .format(\"io.tiledb.spark\")\\\n",
    "  .option(\"uri\", \"study_index_tldb\")\\\n",
    "  .option('schema.dim.0.name', 'study_id')\\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create from pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/study_index_sample.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('study_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('study_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pmid'] = df['pmid'].astype('|S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pmid</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>pub_journal</th>\n",
       "      <th>pub_title</th>\n",
       "      <th>pub_author</th>\n",
       "      <th>trait_reported</th>\n",
       "      <th>ancestry_initial</th>\n",
       "      <th>n_initial</th>\n",
       "      <th>n_cases</th>\n",
       "      <th>trait_category</th>\n",
       "      <th>num_assoc_loci</th>\n",
       "      <th>has_sumstats</th>\n",
       "      <th>trait_efos</th>\n",
       "      <th>ancestry_replication</th>\n",
       "      <th>n_replication</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>study_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GCST000001</th>\n",
       "      <td>b'PMID:15761122'</td>\n",
       "      <td>2005-03-10</td>\n",
       "      <td>Science</td>\n",
       "      <td>Complement factor H polymorphism in age-relate...</td>\n",
       "      <td>Klein RJ</td>\n",
       "      <td>Age-related macular degeneration</td>\n",
       "      <td>[European=146]</td>\n",
       "      <td>146</td>\n",
       "      <td>96.0</td>\n",
       "      <td>Nervous system</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[EFO_0001365]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCST000002</th>\n",
       "      <td>b'PMID:16252231'</td>\n",
       "      <td>2005-09-09</td>\n",
       "      <td>Am J Hum Genet</td>\n",
       "      <td>High-resolution whole-genome association study...</td>\n",
       "      <td>Maraganore DM</td>\n",
       "      <td>Parkinson's disease</td>\n",
       "      <td>[Asian unspecified=1, European=744, Other=141]</td>\n",
       "      <td>886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nervous system</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[EFO_0002508]</td>\n",
       "      <td>[Other=122, Asian unspecified=1, European=541]</td>\n",
       "      <td>664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCST000003</th>\n",
       "      <td>b'PMID:16614226'</td>\n",
       "      <td>2006-04-14</td>\n",
       "      <td>Science</td>\n",
       "      <td>A common genetic variant is associated with ad...</td>\n",
       "      <td>Herbert A</td>\n",
       "      <td>Obesity</td>\n",
       "      <td>[NR=694]</td>\n",
       "      <td>694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Disease</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[EFO_0001073]</td>\n",
       "      <td>[European=10527, African American or Afro-Cari...</td>\n",
       "      <td>11622.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCST000004</th>\n",
       "      <td>b'PMID:16648850'</td>\n",
       "      <td>2006-04-30</td>\n",
       "      <td>Nat Genet</td>\n",
       "      <td>A common genetic variant in the NOS1 regulator...</td>\n",
       "      <td>Arking DE</td>\n",
       "      <td>QT interval</td>\n",
       "      <td>[European=200]</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cardiovascular measurement</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>[EFO_0004682]</td>\n",
       "      <td>[European=4851]</td>\n",
       "      <td>4851.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCST000005</th>\n",
       "      <td>b'PMID:17052657'</td>\n",
       "      <td>2006-09-28</td>\n",
       "      <td>Lancet Neurol</td>\n",
       "      <td>Genome-wide genotyping in Parkinson's disease ...</td>\n",
       "      <td>Fung HC</td>\n",
       "      <td>Parkinson's disease</td>\n",
       "      <td>[European=537]</td>\n",
       "      <td>537</td>\n",
       "      <td>267.0</td>\n",
       "      <td>Nervous system</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[EFO_0002508]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCST000080_3</th>\n",
       "      <td>b'PMID:17903294'</td>\n",
       "      <td>2007-09-19</td>\n",
       "      <td>BMC Med Genet</td>\n",
       "      <td>Genome-wide association and linkage analyses o...</td>\n",
       "      <td>Yang Q</td>\n",
       "      <td>Hemostatic factors and hematological phenotype...</td>\n",
       "      <td>[European=1062]</td>\n",
       "      <td>1062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hematological measurement</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[EFO_0004527]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCST000080_4</th>\n",
       "      <td>b'PMID:17903294'</td>\n",
       "      <td>2007-09-19</td>\n",
       "      <td>BMC Med Genet</td>\n",
       "      <td>Genome-wide association and linkage analyses o...</td>\n",
       "      <td>Yang Q</td>\n",
       "      <td>Hemostatic factors and hematological phenotype...</td>\n",
       "      <td>[European=1062]</td>\n",
       "      <td>1062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Protein measurement</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[EFO_0004792]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCST000080_5</th>\n",
       "      <td>b'PMID:17903294'</td>\n",
       "      <td>2007-09-19</td>\n",
       "      <td>BMC Med Genet</td>\n",
       "      <td>Genome-wide association and linkage analyses o...</td>\n",
       "      <td>Yang Q</td>\n",
       "      <td>Hemostatic factors and hematological phenotype...</td>\n",
       "      <td>[European=1062]</td>\n",
       "      <td>1062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hematological measurement</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[EFO_0004305]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCST000080_6</th>\n",
       "      <td>b'PMID:17903294'</td>\n",
       "      <td>2007-09-19</td>\n",
       "      <td>BMC Med Genet</td>\n",
       "      <td>Genome-wide association and linkage analyses o...</td>\n",
       "      <td>Yang Q</td>\n",
       "      <td>Hemostatic factors and hematological phenotype...</td>\n",
       "      <td>[European=1062]</td>\n",
       "      <td>1062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hematological measurement</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[EFO_0004301]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCST000080_7</th>\n",
       "      <td>b'PMID:17903294'</td>\n",
       "      <td>2007-09-19</td>\n",
       "      <td>BMC Med Genet</td>\n",
       "      <td>Genome-wide association and linkage analyses o...</td>\n",
       "      <td>Yang Q</td>\n",
       "      <td>Hemostatic factors and hematological phenotype...</td>\n",
       "      <td>[European=1062]</td>\n",
       "      <td>1062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Platelet aggregation</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>[GO_0070527]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          pmid    pub_date     pub_journal  \\\n",
       "study_id                                                     \n",
       "GCST000001    b'PMID:15761122'  2005-03-10         Science   \n",
       "GCST000002    b'PMID:16252231'  2005-09-09  Am J Hum Genet   \n",
       "GCST000003    b'PMID:16614226'  2006-04-14         Science   \n",
       "GCST000004    b'PMID:16648850'  2006-04-30       Nat Genet   \n",
       "GCST000005    b'PMID:17052657'  2006-09-28   Lancet Neurol   \n",
       "...                        ...         ...             ...   \n",
       "GCST000080_3  b'PMID:17903294'  2007-09-19   BMC Med Genet   \n",
       "GCST000080_4  b'PMID:17903294'  2007-09-19   BMC Med Genet   \n",
       "GCST000080_5  b'PMID:17903294'  2007-09-19   BMC Med Genet   \n",
       "GCST000080_6  b'PMID:17903294'  2007-09-19   BMC Med Genet   \n",
       "GCST000080_7  b'PMID:17903294'  2007-09-19   BMC Med Genet   \n",
       "\n",
       "                                                      pub_title  \\\n",
       "study_id                                                          \n",
       "GCST000001    Complement factor H polymorphism in age-relate...   \n",
       "GCST000002    High-resolution whole-genome association study...   \n",
       "GCST000003    A common genetic variant is associated with ad...   \n",
       "GCST000004    A common genetic variant in the NOS1 regulator...   \n",
       "GCST000005    Genome-wide genotyping in Parkinson's disease ...   \n",
       "...                                                         ...   \n",
       "GCST000080_3  Genome-wide association and linkage analyses o...   \n",
       "GCST000080_4  Genome-wide association and linkage analyses o...   \n",
       "GCST000080_5  Genome-wide association and linkage analyses o...   \n",
       "GCST000080_6  Genome-wide association and linkage analyses o...   \n",
       "GCST000080_7  Genome-wide association and linkage analyses o...   \n",
       "\n",
       "                 pub_author  \\\n",
       "study_id                      \n",
       "GCST000001         Klein RJ   \n",
       "GCST000002    Maraganore DM   \n",
       "GCST000003        Herbert A   \n",
       "GCST000004        Arking DE   \n",
       "GCST000005          Fung HC   \n",
       "...                     ...   \n",
       "GCST000080_3         Yang Q   \n",
       "GCST000080_4         Yang Q   \n",
       "GCST000080_5         Yang Q   \n",
       "GCST000080_6         Yang Q   \n",
       "GCST000080_7         Yang Q   \n",
       "\n",
       "                                                 trait_reported  \\\n",
       "study_id                                                          \n",
       "GCST000001                     Age-related macular degeneration   \n",
       "GCST000002                                  Parkinson's disease   \n",
       "GCST000003                                              Obesity   \n",
       "GCST000004                                          QT interval   \n",
       "GCST000005                                  Parkinson's disease   \n",
       "...                                                         ...   \n",
       "GCST000080_3  Hemostatic factors and hematological phenotype...   \n",
       "GCST000080_4  Hemostatic factors and hematological phenotype...   \n",
       "GCST000080_5  Hemostatic factors and hematological phenotype...   \n",
       "GCST000080_6  Hemostatic factors and hematological phenotype...   \n",
       "GCST000080_7  Hemostatic factors and hematological phenotype...   \n",
       "\n",
       "                                            ancestry_initial  n_initial  \\\n",
       "study_id                                                                  \n",
       "GCST000001                                    [European=146]        146   \n",
       "GCST000002    [Asian unspecified=1, European=744, Other=141]        886   \n",
       "GCST000003                                          [NR=694]        694   \n",
       "GCST000004                                    [European=200]        200   \n",
       "GCST000005                                    [European=537]        537   \n",
       "...                                                      ...        ...   \n",
       "GCST000080_3                                 [European=1062]       1062   \n",
       "GCST000080_4                                 [European=1062]       1062   \n",
       "GCST000080_5                                 [European=1062]       1062   \n",
       "GCST000080_6                                 [European=1062]       1062   \n",
       "GCST000080_7                                 [European=1062]       1062   \n",
       "\n",
       "              n_cases              trait_category  num_assoc_loci  \\\n",
       "study_id                                                            \n",
       "GCST000001       96.0              Nervous system               1   \n",
       "GCST000002        NaN              Nervous system               0   \n",
       "GCST000003        NaN                     Disease               0   \n",
       "GCST000004        NaN  Cardiovascular measurement               1   \n",
       "GCST000005      267.0              Nervous system               0   \n",
       "...               ...                         ...             ...   \n",
       "GCST000080_3      NaN   Hematological measurement               0   \n",
       "GCST000080_4      NaN         Protein measurement               0   \n",
       "GCST000080_5      NaN   Hematological measurement               0   \n",
       "GCST000080_6      NaN   Hematological measurement               0   \n",
       "GCST000080_7      NaN        Platelet aggregation               0   \n",
       "\n",
       "              has_sumstats     trait_efos  \\\n",
       "study_id                                    \n",
       "GCST000001           False  [EFO_0001365]   \n",
       "GCST000002           False  [EFO_0002508]   \n",
       "GCST000003           False  [EFO_0001073]   \n",
       "GCST000004           False  [EFO_0004682]   \n",
       "GCST000005           False  [EFO_0002508]   \n",
       "...                    ...            ...   \n",
       "GCST000080_3         False  [EFO_0004527]   \n",
       "GCST000080_4         False  [EFO_0004792]   \n",
       "GCST000080_5         False  [EFO_0004305]   \n",
       "GCST000080_6         False  [EFO_0004301]   \n",
       "GCST000080_7         False   [GO_0070527]   \n",
       "\n",
       "                                           ancestry_replication  n_replication  \n",
       "study_id                                                                        \n",
       "GCST000001                                                  NaN            NaN  \n",
       "GCST000002       [Other=122, Asian unspecified=1, European=541]          664.0  \n",
       "GCST000003    [European=10527, African American or Afro-Cari...        11622.0  \n",
       "GCST000004                                      [European=4851]         4851.0  \n",
       "GCST000005                                                  NaN            NaN  \n",
       "...                                                         ...            ...  \n",
       "GCST000080_3                                                NaN            NaN  \n",
       "GCST000080_4                                                NaN            NaN  \n",
       "GCST000080_5                                                NaN            NaN  \n",
       "GCST000080_6                                                NaN            NaN  \n",
       "GCST000080_7                                                NaN            NaN  \n",
       "\n",
       "[97 rows x 15 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.astype({'pmid':'|S'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pmid',\n",
       " 'pub_journal',\n",
       " 'pub_title',\n",
       " 'pub_author',\n",
       " 'trait_reported',\n",
       " 'ancestry_initial',\n",
       " 'trait_category',\n",
       " 'has_sumstats',\n",
       " 'trait_efos',\n",
       " 'ancestry_replication']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [col for col in df.columns if col not in ['pub_date', 'n_initial','n_cases','num_assoc_loci','n_replication']]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cols] = df[cols].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pmid                     object\n",
       "pub_date                 object\n",
       "pub_journal              object\n",
       "pub_title                object\n",
       "pub_author               object\n",
       "trait_reported           object\n",
       "ancestry_initial         object\n",
       "n_initial                 int64\n",
       "n_cases                 float64\n",
       "trait_category           object\n",
       "num_assoc_loci            int64\n",
       "has_sumstats             object\n",
       "trait_efos               object\n",
       "ancestry_replication     object\n",
       "n_replication           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.fillna(0).astype({\"pmid\": str, \"pub_date\": 'datetime64','has_sumstats':str})\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "mixed inferred dtype not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-134-58c628bf7543>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m tiledb.from_pandas(\"study_index_tldb\", df.fillna(''), chunksize = 10**6,\n\u001b[1;32m----> 6\u001b[1;33m                 fillna={\"trait_efos\":'', 'ancestry_replication':'','ancestry_initial':''})\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\tiledb\\lib\\site-packages\\tiledb\\dataframe_.py\u001b[0m in \u001b[0;36mfrom_pandas\u001b[1;34m(uri, dataframe, **kwargs)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m     column_infos = _get_column_infos(\n\u001b[1;32m--> 453\u001b[1;33m         \u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtiledb_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"column_types\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtiledb_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"varlen_types\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m     )\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tiledb\\lib\\site-packages\\tiledb\\dataframe_.py\u001b[0m in \u001b[0;36m_get_column_infos\u001b[1;34m(df, column_types, varlen_types)\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mcolumn_infos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mColumnInfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_types\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvarlen_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m             \u001b[0mcolumn_infos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mColumnInfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvarlen_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    177\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcolumn_infos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tiledb\\lib\\site-packages\\tiledb\\dataframe_.py\u001b[0m in \u001b[0;36mfrom_values\u001b[1;34m(cls, array_like, varlen_types)\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m                 raise NotImplementedError(\n\u001b[1;32m--> 101\u001b[1;33m                     \u001b[1;34mf\"{inferred_dtype} inferred dtype not supported\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m                 )\n\u001b[0;32m    103\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: mixed inferred dtype not supported"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "if tiledb.array_exists(\"study_index_tldb\", isdense=False, issparse=False):\n",
    "    shutil.rmtree('study_index_tldb')\n",
    "    \n",
    "tiledb.from_pandas(\"study_index_tldb\", df.fillna(''), chunksize = 10**6,\n",
    "                fillna={\"trait_efos\":'', 'ancestry_replication':'','ancestry_initial':''})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TileDB from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Spark dataframe to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to convert array data type to string in order to be able to save to csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import StringType\n",
    "\n",
    "# def array_to_string(my_list):\n",
    "#     if my_list:\n",
    "#         return ';'.join([str(elem) for elem in my_list])\n",
    "#     else:\n",
    "#         return ''\n",
    "\n",
    "# array_to_string_udf = udf(array_to_string, StringType())\n",
    "\n",
    "# study_index_ps = study_index.withColumn('trait_efos_str', array_to_string_udf(study_index[\"trait_efos\"]))\n",
    "# study_index_ps = study_index_ps.withColumn('ancestry_replication_str', array_to_string_udf(study_index[\"ancestry_replication\"]))\n",
    "# study_index_ps = study_index_ps.withColumn('ancestry_initial_str', array_to_string_udf(study_index[\"ancestry_initial\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study_index = study_index.drop(\"trait_efos\").drop('ancestry_replication').drop('ancestry_initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------+-------+---------+-------------+--------------+-------------+-------------+----------+--------------+--------------------+----------+--------------------+-------------+--------------------+\n",
      "|    ancestry_initial|ancestry_replication|has_sumstats|n_cases|n_initial|n_replication|num_assoc_loci|         pmid|   pub_author|  pub_date|   pub_journal|           pub_title|  study_id|      trait_category|   trait_efos|      trait_reported|\n",
      "+--------------------+--------------------+------------+-------+---------+-------------+--------------+-------------+-------------+----------+--------------+--------------------+----------+--------------------+-------------+--------------------+\n",
      "|      [European=146]|                null|       false|     96|      146|         null|             1|PMID:15761122|     Klein RJ|2005-03-10|       Science|Complement factor...|GCST000001|      Nervous system|[EFO_0001365]|Age-related macul...|\n",
      "|[Asian unspecifie...|[Other=122, Asian...|       false|   null|      886|          664|             0|PMID:16252231|Maraganore DM|2005-09-09|Am J Hum Genet|High-resolution w...|GCST000002|      Nervous system|[EFO_0002508]| Parkinson's disease|\n",
      "|            [NR=694]|[European=10527, ...|       false|   null|      694|        11622|             0|PMID:16614226|    Herbert A|2006-04-14|       Science|A common genetic ...|GCST000003|             Disease|[EFO_0001073]|             Obesity|\n",
      "|      [European=200]|     [European=4851]|       false|   null|      200|         4851|             1|PMID:16648850|    Arking DE|2006-04-30|     Nat Genet|A common genetic ...|GCST000004|Cardiovascular me...|[EFO_0004682]|         QT interval|\n",
      "|      [European=537]|                null|       false|    267|      537|         null|             0|PMID:17052657|      Fung HC|2006-09-28| Lancet Neurol|Genome-wide genot...|GCST000005|      Nervous system|[EFO_0002508]| Parkinson's disease|\n",
      "+--------------------+--------------------+------------+-------+---------+-------------+--------------+-------------+-------------+----------+--------------+--------------------+----------+--------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "study_index.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_index.write.csv(\"study_index_csv\",header=True,mode='overwrite')\n",
    "# study_index.write.format('com.databricks.spark.csv') \\\n",
    "#             .save(\"study_index_csv\",header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TileDB Array from CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For attributes that you need to use with QueryCondition, the data type needs to be specified as \"ascii\". This can be done by passing a dictionary with type overrides to the `column_types` keyword argument of `from_csv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"ascii\" requires TileDB-Py version 0.10.3 or 0.10.4. For earlier versions, you could also use {\"study_id\": bytes} -- this will translate into a non-string type in the future, however. \"ascii\" will ensure that the underlying array schema is marked as text (TILEDB_ASCII)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['study_index_csv\\\\part-00000-b47b2890-815d-4e81-adc6-e321b4eb9e1e-c000.csv']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "study_index_csv = sorted(glob.glob('study_index_csv/*.csv'))\n",
    "study_index_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "if tiledb.array_exists(\"study_index_tldb\", isdense=False, issparse=False):\n",
    "    shutil.rmtree('study_index_tldb')\n",
    "    \n",
    "tiledb.from_csv(\"study_index_tldb\", 'study_index.csv', chunksize = 10**6,\n",
    "                fillna={\"trait_efos\":'', 'ancestry_replication':'','ancestry_initial':''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TileDBError",
     "evalue": "Multiple input CSV files requires a 'chunksize' argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTileDBError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-a2f68301c461>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                 \u001b[0mfillna\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"trait_efos\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ancestry_replication'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ancestry_initial'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mcolumn_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"trait_category\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                 \u001b[0mvarlen_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'trait_category'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                )\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\tiledb\\lib\\site-packages\\tiledb\\dataframe_.py\u001b[0m in \u001b[0;36mfrom_csv\u001b[1;34m(uri, csv_file, **kwargs)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_file\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"schema_only\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 711\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTileDBError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Multiple input CSV files requires a 'chunksize' argument\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTileDBError\u001b[0m: Multiple input CSV files requires a 'chunksize' argument"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "if tiledb.array_exists(\"study_index_tldb\", isdense=False, issparse=False):\n",
    "    shutil.rmtree('study_index_tldb')\n",
    "\n",
    "tiledb.from_csv(\"study_index_tldb\", \n",
    "                study_index_csv,\n",
    "                index_dims = ['study_id'],\n",
    "                chunksize = 10**6,\n",
    "                fillna={\"trait_efos\":'', 'ancestry_replication':'','ancestry_initial':''},\n",
    "                column_types = {\"trait_category\": bytes},\n",
    "                varlen_types={'trait_category'}\n",
    "\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArraySchema(\n",
       "  domain=Domain(*[\n",
       "    Dim(name='study_id', domain=(None, None), tile='None', dtype='|S0'),\n",
       "  ]),\n",
       "  attrs=[\n",
       "    Attr(name='ancestry_initial', dtype='<U0', var=True, nullable=False),\n",
       "    Attr(name='ancestry_replication', dtype='<U0', var=True, nullable=False),\n",
       "    Attr(name='has_sumstats', dtype='uint8', var=False, nullable=False),\n",
       "    Attr(name='n_cases', dtype='float64', var=False, nullable=False),\n",
       "    Attr(name='n_initial', dtype='int64', var=False, nullable=False),\n",
       "    Attr(name='n_replication', dtype='float64', var=False, nullable=False),\n",
       "    Attr(name='num_assoc_loci', dtype='int64', var=False, nullable=False),\n",
       "    Attr(name='pmid', dtype='<U0', var=True, nullable=False),\n",
       "    Attr(name='pub_author', dtype='<U0', var=True, nullable=False),\n",
       "    Attr(name='pub_date', dtype='<U0', var=True, nullable=False),\n",
       "    Attr(name='pub_journal', dtype='<U0', var=True, nullable=False),\n",
       "    Attr(name='pub_title', dtype='<U0', var=True, nullable=False),\n",
       "    Attr(name='trait_category', dtype='|S0', var=True, nullable=False),\n",
       "    Attr(name='trait_efos', dtype='<U0', var=True, nullable=False),\n",
       "    Attr(name='trait_reported', dtype='<U0', var=True, nullable=False),\n",
       "  ],\n",
       "  cell_order='row-major',\n",
       "  tile_order='row-major',\n",
       "  capacity=10000,\n",
       "  sparse=True,\n",
       "  allows_duplicates=True,\n",
       "  coords_filters=FilterList([ZstdFilter(level=-1)]),\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = tiledb.ArraySchema.load('study_index_tldb')\n",
    "schema"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tiledb]",
   "language": "python",
   "name": "conda-env-tiledb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "338.987px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
